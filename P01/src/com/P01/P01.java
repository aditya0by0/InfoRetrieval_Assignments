package com.P01;
import org.apache.lucene.analysis.custom.CustomAnalyzer;
import org.apache.lucene.analysis.en.PorterStemFilterFactory;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.core.StopAnalyzer;
import org.apache.lucene.analysis.core.WhitespaceAnalyzer;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;

import java.io.IOException;
import java.nio.file.Paths;
import java.nio.file.Path;

public class P01 {
	
	private static final String TEXT = "Today is sunny. She is a sunny girl. To be or not to be. She is in Berlin today. Sunny Berlin! Berlin is always exciting!";
	
	public static void main(String[] args) throws IOException {		
		
        System.out.println("Programming Assignment 01:");

        // Get working dir path & append Stopwords file path to it: 
    	Path CurrentDirectory = Paths.get(System.getProperty("user.dir"));
        Path StopWordsPath = CurrentDirectory.resolve("src").resolve("resource");
        Path StopWordsAbsPath = CurrentDirectory.resolve("src").resolve("resource").resolve("stopwords.txt");
        
		// Sub-part 'a' - Call to Tokenizer - Standard and Whitespace:
		System.out.println("\nTask 'a':");
		taskA();
		
		// Sub-part 'b' - Call to Standard Tokenizer and Stopword Filter:
		System.out.println("\nTask 'b':");
		taskB(StopWordsAbsPath);
		
		// Sub-part 'c' - Call to Custom Analyser with the given configuration:
		System.out.println("\nTask 'c':");
		taskC(StopWordsPath);
    }
    
    public static void analyze(Analyzer analyzer, String input) throws IOException {
        
        TokenStream ts = analyzer.tokenStream("field", input);
        CharTermAttribute charTermAttribute = ts.addAttribute(CharTermAttribute.class);

        ts.reset();
        
        while (ts.incrementToken()) {
            System.out.println(charTermAttribute.toString());
        }

        ts.end();
        ts.close();
    }
    
    public static void taskA() throws IOException {
        
        Analyzer standardAnalyzer = new StandardAnalyzer();
        Analyzer whitespaceAnalyzer = new WhitespaceAnalyzer();

        System.out.println("\nTokens generated by Standard Tokenizer:");
        analyze(standardAnalyzer, TEXT);
        
        System.out.println("\nTokens generated by Whitespace Tokenizer:");
        analyze(whitespaceAnalyzer, TEXT);
    }
    
    public static void taskB(Path StopWordsPath) throws IOException {
        
        Analyzer standardAnalyzer = new StandardAnalyzer();
        Analyzer stopAnalyzer = new StopAnalyzer(StopWordsPath);
        
        System.out.println("\nTokens generated by Standard Tokenizer:");
        analyze(standardAnalyzer, TEXT);
        
        System.out.println("\nTokens generated by own Stopwords Filter:");
        analyze(stopAnalyzer, TEXT);
    }
    
    public static void taskC(Path StopWordsPath) throws IOException{
    	
        // Custom Analyser with given configuration:
        CustomAnalyzer customAnalyzer = CustomAnalyzer.builder(StopWordsPath)
        		.withTokenizer("standard")
        		.addTokenFilter("lowercase")
        		.addTokenFilter("stop", "ignoreCase", "false", "format", "wordset", "words", "stopwords.txt")
        		.addTokenFilter(PorterStemFilterFactory.class)
        		.build();
        
        System.out.println("\nTokens generated by Custom Analyzer:");
        analyze(customAnalyzer, TEXT);
    }
}