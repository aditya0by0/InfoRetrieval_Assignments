package com.P02;

import java.io.File;
import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.nio.file.FileSystems;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.Arrays;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import org.apache.commons.io.FileUtils;
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.core.LowerCaseFilterFactory;
import org.apache.lucene.analysis.custom.CustomAnalyzer;
import org.apache.lucene.analysis.standard.StandardTokenizerFactory;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.document.FieldType;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexOptions;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.index.IndexWriterConfig.OpenMode;
import org.apache.lucene.index.PostingsEnum;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.Terms;
import org.apache.lucene.index.TermsEnum;
import org.apache.lucene.queryparser.classic.ParseException;
import org.apache.lucene.queryparser.classic.QueryParser;
import org.apache.lucene.search.DocIdSetIterator;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.ScoreDoc;
import org.apache.lucene.search.TopDocs;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.util.BytesRef;

public class P02 {
	
	public static void main(String args[]) throws IOException, ParseException {
		
		Path indexPath = FileSystems.getDefault().getPath("resources/index");
		File dataDir = new File("resources/data");
		Analyzer analyzer = CustomAnalyzer.builder(Paths.get("src/com/P02/"))
				.withTokenizer(StandardTokenizerFactory.class).addTokenFilter(LowerCaseFilterFactory.class).build();

		Directory directory = FSDirectory.open(indexPath);
		// do indexing of documents
		executeindexing(directory, analyzer, dataDir);

		// search setup
		Set<String> tokens = new HashSet<String>();
		DirectoryReader ireader = DirectoryReader.open(directory);
		IndexSearcher isearcher = new IndexSearcher(ireader);
		QueryParser parser = new QueryParser("contents", analyzer);

		tokens = getAllTokens(ireader);
		// print posting list for each token
		printPostingList(tokens, parser, ireader, isearcher);
		
		System.out.println();
		System.out.println("For query: sunny AND to =");
		
		String query = "sunny to";
		termIntersection(query, parser, ireader, isearcher);
		
		ireader.close();
		directory.close();

	}

	private static void termIntersection(String word, QueryParser parser, DirectoryReader ireader, IndexSearcher isearcher) throws ParseException, IOException {
		// TODO Auto-generated method stub
		parser.setDefaultOperator(QueryParser.Operator.AND);
		Query query = parser.parse(word);
		// iterate over words from here
		TopDocs topDocss = isearcher.search(query, 1000);
		ScoreDoc[] hits = topDocss.scoreDocs;
		System.out.println(hits.length+" documents have both tokens" );
		
		List<String> wordss = Arrays.asList(word.split(" "));
		for (int i = 0; i < hits.length; i++) {
			int docId = hits[i].doc;
			Terms terms = ireader.getTermVector(docId, "contents");
			TermsEnum te = terms.iterator();
			System.out.print("[" + docId);
			
			PostingsEnum docsAndPosEnum = null;
			BytesRef bytesRef;
			while ((bytesRef = te.next()) != null) {
				docsAndPosEnum = te.postings(docsAndPosEnum, PostingsEnum.ALL);
				// for each term (iterator next) in this field (field)
				// iterate over the docs (should only be one)
				int nextDoc = docsAndPosEnum.nextDoc();
				assert nextDoc != DocIdSetIterator.NO_MORE_DOCS;
				final int fr = docsAndPosEnum.freq();
				final int p = docsAndPosEnum.nextPosition();
				final int o = docsAndPosEnum.startOffset();
				String curWord = bytesRef.utf8ToString();
				
				if (wordss.contains(curWord)) {
					System.out.print(":" + fr);
					
					if (fr == 1) {
						System.out.print(":[" + o + "]]");
					}
					else if (fr > 1) {
						System.out.print(":[" + o);
						
						for (int k = 0; k < fr - 1; k++) {
							int np = docsAndPosEnum.nextPosition();
							int offstart = docsAndPosEnum.startOffset();
							System.out.print("," + offstart);
						}
						System.out.print("]]");
					}
				}
			}
			if (i != hits.length - 1)
				System.out.print("->");
		}
	}

	private static void printPostingList(Set<String> tokens, QueryParser parser, DirectoryReader ireader, IndexSearcher isearcher) throws ParseException, IOException {
		// TODO Auto-generated method stub
		for (String word : tokens) {

			Query query = parser.parse(word);

			TopDocs topDocs = isearcher.search(query, 1000);
			ScoreDoc[] hits = topDocs.scoreDocs;

			Term term = new Term("contents", word);
			System.out.print("[" + word + ":");
			System.out.print(ireader.totalTermFreq(term));
			System.out.print(":" + ireader.docFreq(term) + "]-->");

			// ITERATE OVER EACH DOCUMENT
			for (int i = 0; i < hits.length; i++) {
				int docId = hits[i].doc;
				Terms terms = ireader.getTermVector(docId, "contents");

				TermsEnum te = terms.iterator();
				System.out.print("[" + docId);

				PostingsEnum docsAndPosEnum = null;
				BytesRef bytesRef;
				while ((bytesRef = te.next()) != null) {
					docsAndPosEnum = te.postings(docsAndPosEnum, PostingsEnum.ALL);
					// for each term (iterator next) in this field (field)
					// iterate over the docs (should only be one)
					int nextDoc = docsAndPosEnum.nextDoc();
					assert nextDoc != DocIdSetIterator.NO_MORE_DOCS;
					final int fr = docsAndPosEnum.freq();
					final int p = docsAndPosEnum.nextPosition();
					final int o = docsAndPosEnum.startOffset();
//	            System.out.println("p="+ p + ", o=" + o + ", l=" + bytesRef.length + ", f=" + fr + ", s=" + bytesRef.utf8ToString());
					String curWord = bytesRef.utf8ToString();
					if (curWord.equals(word)) {
						System.out.print(":" + fr);
						if (fr == 1)
							System.out.print(":[" + o + "]]");
						else if (fr > 1) {
							System.out.print(":[" + o);
							
							for (int k = 0; k < fr - 1; k++) {
								int np = docsAndPosEnum.nextPosition();
								int offstart = docsAndPosEnum.startOffset();
								System.out.print("," + offstart);
							}
							
							System.out.print("]]");
						}
					}
				}
				if (i != hits.length - 1) {
					System.out.print("->");
				}

			}
			System.out.println();
		}
	}

	private static Set<String> getAllTokens(DirectoryReader ireader) throws IOException {
		// TODO Auto-generated method stub
		Set<String> tokens = new HashSet<String>();
		int numDocs = ireader.numDocs();
		for (int i = 0; i < numDocs; i++) {
			Terms vector = ireader.getTermVector(i, "contents");
			TermsEnum trms = vector.iterator();
			BytesRef ter;
			while ((ter = trms.next()) != null) {

				String termstr = ter.utf8ToString();
				tokens.add(termstr);
			}
		}
		return tokens;
	}

	private static void executeindexing(Directory directory, Analyzer analyzer, File dataDir) throws IOException {
		// TODO Auto-generated method stub

		IndexWriterConfig config = new IndexWriterConfig(analyzer);
		config.setOpenMode(OpenMode.CREATE);
		IndexWriter iwriter = new IndexWriter(directory, config);

		FieldType fieldType = new FieldType();
		fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
		fieldType.setStored(true); // default = false (same as Field.Store.NO)
		fieldType.setTokenized(true); // default = true (tokenize the content)
		fieldType.setStoreTermVectorOffsets(true);
		fieldType.setStoreTermVectorPayloads(true);
		fieldType.setStoreTermVectorPositions(true);
		fieldType.setStoreTermVectors(true);
		
		File[] files = dataDir.listFiles();
		for (int i = 0; i < files.length; i++) {
			File f = files[i];
			Document doc = new Document();
//            doc.add(new TextField("contents", new FileReader(f)));
			String text = FileUtils.readFileToString(f, StandardCharsets.UTF_8);
//    		System.out.println(text);
			doc.add(new Field("contents", text, fieldType));
			iwriter.addDocument(doc);
		}
		iwriter.close();
	}
}